general:
  use_uvloop: true
  front_end:
    _type: fastapi
    endpoints:
      - path: /generate_query
        method: POST
        description: Creates the query
        function_name: generate_query
      - path: /generate_summary
        method: POST
        description: Generates the summary
        function_name: generate_summary
      - path: /artifact_qa
        method: POST
        description: Q/A or chat about a previously generated artifact
        function_name: artifact_qa
      - path: /aiqhealth
        method: GET
        description: Health check for the AIQ AIRA service
        function_name: health_check
      - path: /default_collections
        method: GET
        description: Get the default collections
        function_name: default_collections

llms:
  # The inst_llm is used for Q&A and report writing and should be an instruct model.
  # The default configuration below is assuming a docker compose deployment of AIRA 
  # that uses local NVIDIA NIM microservices.
  # Update if you are deploying differently or using hosted NVIDIA NIM microservices.
  instruct_llm:
    _type: openai
    model_name: meta/llama-3.3-70b-instruct
    temperature: 0.0
    base_url: https://integrate.api.nvidia.com/v1
    api_key: ${OPENAI_API_KEY}
    
  # The reasoning llm is used for report planning and reflection and should be a reasoning model
  # that supports thinking tokens. The default configuration below is used assuming a docker compose
  # deployment of AIRA and RAG with a local NVIDIA NIM microservice for the nemotron model.
  # Update if you are deploying differently or using hosted NVIDIA NIM microservices.
  nemotron:
    _type: openai
    model_name : nvidia/llama-3.3-nemotron-super-49b-v1
    temperature: 0.5
    base_url: https://integrate.api.nvidia.com/v1
    max_tokens: 5000
    stream: true
    api_key: ${OPENAI_API_KEY}
  

functions:
  generate_query:
    _type: generate_queries

  generate_summary:
    _type: generate_summaries
    # update to the IP address of the RAG server if you are not deploying RAG with docker compose
    rag_url: http://nvidia-api:8000/v1

  artifact_qa:
    _type: artifact_qa
    llm_name: instruct_llm
    # update to the IP address of the RAG server if you are not deploying RAG with docker compose
    rag_url: http://nvidia-api:8000/v1
  
  health_check:
    _type: health_check

  default_collections:
    _type: default_collections
    collections:
      - name: "Biomedical_Dataset"
        topic: "Biomedical"
        report_organization: "You are a medical researcher who specializes in cystic fibrosis. Create a report analyzing how CFTR modulators can be used to restore CFTR protein functions. Include a 150-200 word abstract and a methods, results, and discussion section. Format your answer in paragraphs. Consider all (and only) relevant data. Give a factual report with cited sources."
      - name: "Financial_Dataset"
        topic: "Financial"
        report_organization: "You are a financial analyst who specializes in financial statement analysis. Write a financial report analyzing the 2023 financial performance of Amazon. Identify trends in revenue growth, net income, and total assets. Discuss how these trends affected Amazon's yearly financial performance for 2023. Your output should be organized into a brief introduction, as many sections as necessary to create a comprehensive report, and a conclusion. Format your answer in paragraphs. Use factual sources such as Amazon's quarterly meeting releases for 2023. Cross analyze the sources to draw original and sound conclusions and explain your reasoning for arriving at conclusions. Do not make any false or unverifiable claims. I want a factual report with cited sources."

workflow:
  _type: ai_researcher